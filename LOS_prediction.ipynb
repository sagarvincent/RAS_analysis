{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# First Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import join\n",
    "import sklearn\n",
    "\n",
    "# specify the path to file\n",
    "joiner = join.joiner(\"D:\\Projects\\Data Analysis\\Robot Assisted surgery EDA\\RCS_X_Xi_metrics_report_with_LoS.xlsx\")\n",
    "sheets = joiner.get_sheets()\n",
    "metrics = sheets['X_Xi_metrics_report']\n",
    "los_data = sheets['X_Xi_counts']\n",
    "# - rename the session identifier col so tha tboth data set have the same col name.\n",
    "metrics.rename(columns={'session_name': 'session_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - data feature columns of interest\n",
    "feature_col = ['session_name','proc_type','task_name','task_id','time_start','time_stop','evtcnt_camera_control','evtcnt_head_out','energy_totalduration','eom_usm1','eom_usm2','eom_usm3','eom_usm4']\n",
    "metrics = metrics[feature_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dataframes for hysterectomy and lobectomy using boolean indexing\n",
    "suregeries = ['hysterectomy', 'lobectomy']\n",
    "\n",
    "# Using boolean indexing to filter the DataFrame\n",
    "metrics = metrics[metrics['proc_type'].isin(suregeries)]\n",
    "los_data = los_data[los_data['proc_type'].isin(suregeries)]\n",
    "print(f\"Surgereis in metrics :{metrics['proc_type'].unique()}\")   # - confirm that only the metioned surgeries are considered\n",
    "print(f\"Surgeries in los sheet:{los_data['proc_type'].unique()}\") # - confirm that only the metioned surgeries are considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - get total task duration column from the start and end time.\n",
    "metrics['time_start'] = pd.to_timedelta(metrics['time_start'])\n",
    "metrics['time_stop'] = pd.to_timedelta(metrics['time_stop'])\n",
    "metrics['task_duration'] = (metrics['time_stop'] - metrics['time_start']).dt.total_seconds()\n",
    "\n",
    "# Rearrange column order\n",
    "columns = list(metrics.columns)\n",
    "columns.remove('task_duration')\n",
    "columns.insert(5, 'task_duration')\n",
    "columns.remove('time_start')\n",
    "columns.remove('time_stop')\n",
    "metrics = metrics[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframes based for different surgereis\n",
    "hysterectomy_metrics = metrics[metrics['proc_type'].isin(['hysterectomy'])]\n",
    "hysterectomy_los = los_data[los_data['proc_type'].isin(['hysterectomy'])]\n",
    "lobectomy_metrics = metrics[metrics['proc_type'].isin(['lobectomy'])]\n",
    "lobectomy_los = los_data[los_data['proc_type'].isin(['lobectomy'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hysterectomy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groub by session id,task name and task id\n",
    "hysterectomy_metrics = hysterectomy_metrics.drop(columns=['proc_type'])\n",
    "hysterectomy_metrics = hysterectomy_metrics.groupby(['session_name','task_name','task_id']).sum().reset_index()\n",
    "print(f\"The no. of unique surgeical cases in metrics set is :{len(hysterectomy_metrics['session_name'].unique())}\")\n",
    "print(f\"The no. of unique tasks present in the entire set is :{len(hysterectomy_metrics['task_name'].unique())}\")\n",
    "print(f\"The frequency tasks is as follows :{hysterectomy_metrics['task_name'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider all the tasks as pivotal to the surgery, the dimensional spac ewould be very high. Given the dataset has limited no. of feature vectors(unique surgical sessions) high  dimensionality would make it even harder to derive relations. One option is to omit the taks labeled as optional. Therefore the non option tasks would be 13. But there are certain optional taks is often perfomed(high frequency of occurence.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique session IDs\n",
    "session_ids = hysterectomy_metrics['session_name'].unique()\n",
    "\n",
    "# List of hysterectomy tasks\n",
    "hysterectomy_tasks = ['division of the broad ligament (right side)', 'bladder flap creation',\n",
    "                      'dissection of ip ligament (right side) (optional)', 'division of the broad ligament (left side)',\n",
    "                      'colpotomy', 'division of uterine vessels (right side)', 'removal of the uterus',\n",
    "                      'vaginal cuff closure', 'dissection of ip ligament (left side) (optional)',\n",
    "                      'division of the round ligament (left side)', 'division of the round ligament (right side)',\n",
    "                      'division of uterine vessels (left side)', 'mobilize colon / removal of adhesions (optional)']\n",
    "\n",
    "# Create a DataFrame with all combinations of session IDs and tasks\n",
    "all_tasks_df = pd.DataFrame([(sid, task) for sid in session_ids for task in hysterectomy_tasks],\n",
    "                            columns=['session_name', 'task_name'])\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(all_tasks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardised the procedure set\n",
    "merged_df = pd.merge(all_tasks_df, hysterectomy_metrics, on=['session_name', 'task_name'], how='left')\n",
    "merged_df.fillna(value=np.nan, inplace=True)\n",
    "merged_df = merged_df.groupby(['session_name', 'task_name']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise using task duration and drop task duration column\n",
    "columns_to_normalize = ['evtcnt_camera_control', 'evtcnt_head_out', 'energy_totalduration',\n",
    "                        'eom_usm1', 'eom_usm2', 'eom_usm3', 'eom_usm4']\n",
    "for col in columns_to_normalize:\n",
    "    merged_df[col] = merged_df[col] / merged_df['task_duration']\n",
    "merged_df = merged_df.drop(columns='task_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure to have sinlge row for each session\n",
    "def restructure(urology):\n",
    "    # restructure data frame, such that each row is a single session id\n",
    "    result = pd.DataFrame()\n",
    "    result_temp = None\n",
    "    i = 0\n",
    "    counter = 0\n",
    "    col_pointer = 0\n",
    "    for session_id in urology['session_name']:\n",
    "        if session_id == urology.iloc[col_pointer,0]:\n",
    "            if i == col_pointer:\n",
    "                result_temp = pd.DataFrame()\n",
    "                df2 = pd.DataFrame(urology.iloc[i,:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            else:\n",
    "                df2 = pd.DataFrame(urology.iloc[i,2:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            i = i +1\n",
    "        else:\n",
    "            result.reset_index(drop=True, inplace=True)\n",
    "            result = pd.concat([result,pd.DataFrame(result_temp)],axis = 0)\n",
    "            col_pointer = i\n",
    "            if i == col_pointer:\n",
    "                result_temp = pd.DataFrame()\n",
    "                df2 = pd.DataFrame(urology.iloc[i,:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            else:\n",
    "                df2 = pd.DataFrame(urology.iloc[i,2:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            i = i +1\n",
    "    return result\n",
    "\n",
    "restruct_df = restructure(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with los_data\n",
    "hysterectomy_los = hysterectomy_los[['session_name','Length of Stay (days)']]\n",
    "hysterectomy_dataset = pd.merge(restruct_df, hysterectomy_los, on=['session_name'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lobectomy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lobectomy_metrics.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groub by session id,task name and task id\n",
    "lobectomy_metrics = lobectomy_metrics.drop(columns=['proc_type'])\n",
    "lobectomy_metrics = lobectomy_metrics.groupby(['session_name','task_name','task_id']).sum().reset_index()\n",
    "print(f\"The no. of unique surgeical cases in metrics set is :{len(lobectomy_metrics['session_name'].unique())}\")\n",
    "print(f\"The no. of unique tasks present in the entire set is :{len(lobectomy_metrics['task_name'].unique())}\")\n",
    "print(f\"The frequency tasks is as follows :{lobectomy_metrics['task_name'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique session IDs\n",
    "session_ids = lobectomy_metrics['session_name'].unique()\n",
    "\n",
    "# List of hysterectomy tasks\n",
    "lobectomy_tasks = [\n",
    "    'Dissect Arteries (Lobe-Specific)',\n",
    "    'Dissect Lymph Nodes (Excluding Subcarinal and Mediastinal)',\n",
    "    'Dissect Bronchus (Lobe Specific)',\n",
    "    'Dissect Veins (Lobe Specific)',\n",
    "    'Divide Inferior Pulmonary Ligament',\n",
    "    'Dissect Station 7 Lymph Node (Subcarinal)',\n",
    "    'Mediastinal Lymph Node Dissection (Station 2,4 on R or 5, 6 on L)',\n",
    "    'Hilar Dissection',\n",
    "    'Fissure Dissection',\n",
    "    'Divide Arteries (Lobe Specific)',\n",
    "    'Divide Veins (Lobe Specific)',\n",
    "    'Divide Bronchus',\n",
    "    'divide arteries (lobe-specific)',\n",
    "    'fissure dissection',\n",
    "    'divide bronchus',\n",
    "    'divide veins (lobe-specific)'\n",
    "]\n",
    "\n",
    "# Create a DataFrame with all combinations of session IDs and tasks\n",
    "all_tasks_df = pd.DataFrame([(sid, task) for sid in session_ids for task in lobectomy_tasks],\n",
    "                            columns=['session_name', 'task_name'])\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(all_tasks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardised the procedure set\n",
    "merged_df = pd.merge(all_tasks_df, lobectomy_metrics, on=['session_name', 'task_name'], how='left')\n",
    "merged_df.fillna(value=np.nan, inplace=True)\n",
    "merged_df = merged_df.groupby(['session_name', 'task_name']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise using task duration and drop task duration column\n",
    "columns_to_normalize = ['evtcnt_camera_control', 'evtcnt_head_out', 'energy_totalduration',\n",
    "                        'eom_usm1', 'eom_usm2', 'eom_usm3', 'eom_usm4']\n",
    "for col in columns_to_normalize:\n",
    "    merged_df[col] = merged_df[col] / merged_df['task_duration']\n",
    "merged_df = merged_df.drop(columns='task_duration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure to have sinlge row for each session\n",
    "def restructure(urology):\n",
    "    # restructure data frame, such that each row is a single session id\n",
    "    result = pd.DataFrame()\n",
    "    result_temp = None\n",
    "    i = 0\n",
    "    counter = 0\n",
    "    col_pointer = 0\n",
    "    for session_id in urology['session_name']:\n",
    "        if session_id == urology.iloc[col_pointer,0]:\n",
    "            if i == col_pointer:\n",
    "                result_temp = pd.DataFrame()\n",
    "                df2 = pd.DataFrame(urology.iloc[i,:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            else:\n",
    "                df2 = pd.DataFrame(urology.iloc[i,2:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            i = i +1\n",
    "        else:\n",
    "            result.reset_index(drop=True, inplace=True)\n",
    "            result = pd.concat([result,pd.DataFrame(result_temp)],axis = 0)\n",
    "            col_pointer = i\n",
    "            if i == col_pointer:\n",
    "                result_temp = pd.DataFrame()\n",
    "                df2 = pd.DataFrame(urology.iloc[i,:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            else:\n",
    "                df2 = pd.DataFrame(urology.iloc[i,2:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            i = i +1\n",
    "    return result\n",
    "\n",
    "restruct_df = restructure(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with los_data\n",
    "los_data = los_data[['session_name','Length of Stay (days)']]\n",
    "lobectomy_dataset = pd.merge(restruct_df, los_data, on=['session_name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the task_id values are identical for the same task (fill the taskid col with highest occuring value)\n",
    "for col in range(len(hysterectomy_dataset.columns)):\n",
    "    if hysterectomy_dataset.columns[col] == \"task_id\":\n",
    "        value_counts = hysterectomy_dataset.iloc[:,col].value_counts()\n",
    "        most_common_value = value_counts.idxmax()\n",
    "        if most_common_value == 0:\n",
    "            most_common_value = hysterectomy_dataset.iloc[:,col].max()\n",
    "        hysterectomy_dataset.iloc[:,col] = most_common_value\n",
    "for col in range(len(lobectomy_dataset.columns)):\n",
    "    if lobectomy_dataset.columns[col] == \"task_id\":\n",
    "        value_counts = lobectomy_dataset.iloc[:,col].value_counts()\n",
    "        most_common_value = value_counts.idxmax()\n",
    "        if most_common_value == 0.0:\n",
    "            most_common_value = lobectomy_dataset.iloc[:,col].max()\n",
    "        lobectomy_dataset.iloc[:,col] = most_common_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_relset = hysterectomy_dataset.drop(columns = ['task_name','evtcnt_camera_control', 'evtcnt_head_out', 'energy_totalduration',\n",
    "                        'eom_usm1', 'eom_usm2', 'eom_usm3', 'eom_usm4'])\n",
    "col_h = task_relset.columns.tolist()\n",
    "for i in range(len(col_h)):\n",
    "    if col_h[i] == \"task_id\":\n",
    "        value_counts = task_relset.iloc[:,i].value_counts()\n",
    "        most_common_value = value_counts.idxmax()\n",
    "        if most_common_value == 0:\n",
    "            most_common_value = task_relset.iloc[:,col].max()\n",
    "        col_h[i] = f\"{most_common_value}\" + \"task_id\"\n",
    "task_relset.columns = col_h\n",
    "def replace_nonzero_with_one_and_zero_with_zero(x):\n",
    "    if x != 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "selected_columns = [col for col in task_relset.columns if col != 'Length of Stay (days)']\n",
    "los = task_relset['Length of Stay (days)']\n",
    "task_relset = task_relset[selected_columns].map(replace_nonzero_with_one_and_zero_with_zero)\n",
    "task_relset = pd.concat([task_relset,los],axis=1)\n",
    "print(task_relset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure each column in dataframe is uniquely named\n",
    "col_h = hysterectomy_dataset.columns.tolist()\n",
    "for i in range(len(col_h)):\n",
    "    if col_h[i] == \"task_id\":\n",
    "        value_counts = hysterectomy_dataset.iloc[:,i].value_counts()\n",
    "        most_common_value = value_counts.idxmax()\n",
    "        col_h[i] = f\"{most_common_value}\" + \"task_id\"\n",
    "        col_h[i+1] = f\"{most_common_value}\" + \"evtcnt_camera_control\"\n",
    "        col_h[i+2] = f\"{most_common_value}\" + \"evtcnt_head_out\"\n",
    "        col_h[i+3] = f\"{most_common_value}\" + \"energy_totalduration\"\n",
    "        col_h[i+4] = f\"{most_common_value}\" + \"eom_usm1\"\n",
    "        col_h[i+5] = f\"{most_common_value}\" + \"eom_usm2\"\n",
    "        col_h[i+6] = f\"{most_common_value}\" + \"eom_usm3\"\n",
    "        col_h[i+7] = f\"{most_common_value}\" + \"eom_usm4\"\n",
    "hysterectomy_dataset.columns = col_h\n",
    "col_l = lobectomy_dataset.columns.tolist()\n",
    "for i in range(len(col_l)):\n",
    "    if col_l[i] == \"task_id\":\n",
    "        value_counts = lobectomy_dataset.iloc[:,i].value_counts()\n",
    "        most_common_value = value_counts.idxmax()\n",
    "        col_l[i] = f\"{most_common_value}\" + \"task_id\"\n",
    "        col_l[i+1] = f\"{most_common_value}\" + \"evtcnt_camera_control\"\n",
    "        col_l[i+2] = f\"{most_common_value}\" + \"evtcnt_head_out\"\n",
    "        col_l[i+3] = f\"{most_common_value}\" + \"energy_totalduration\"\n",
    "        col_l[i+4] = f\"{most_common_value}\" + \"eom_usm1\"\n",
    "        col_l[i+5] = f\"{most_common_value}\" + \"eom_usm2\"\n",
    "        col_l[i+6] = f\"{most_common_value}\" + \"eom_usm3\"\n",
    "        col_l[i+7] = f\"{most_common_value}\" + \"eom_usm4\"\n",
    "lobectomy_dataset.columns = col_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"excel_writer = pd.ExcelWriter('output.xlsx', engine='xlsxwriter')\n",
    "hysterectomy_dataset.to_excel(excel_writer, sheet_name='hysterectomy_dataset', index=False)\n",
    "lobectomy_dataset.to_excel(excel_writer, sheet_name='lobectomy_dataset', index=False)\n",
    "# Save the Excel file\n",
    "excel_writer.close()\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# define datasets\n",
    "print(hysterectomy_dataset.info())\n",
    "lin_hysterectomy = hysterectomy_dataset\n",
    "print(lobectomy_dataset.info())\n",
    "lin_lobectomy = lobectomy_dataset\n",
    "# convert the columns to integers and strings\n",
    "strings = [r'session_name',r'task_name',r'task_id']\n",
    "floats = [r'evtcnt_camera_control',r'evtcnt_head_out',r'energy_totalduration',r'eom_usm1',r'eom_usm2',r'eom_usm3',r'eom_usm4']\n",
    "\n",
    "#convert dtypes\n",
    "for column in hysterectomy_dataset.columns:\n",
    "    for patterns in strings:\n",
    "        if re.search(patterns, column):\n",
    "            hysterectomy_dataset[column] = hysterectomy_dataset[column].astype('string')\n",
    "    for patternf in floats:\n",
    "        if re.search(patternf, column):\n",
    "            hysterectomy_dataset[column] = pd.to_numeric(hysterectomy_dataset[column], errors='coerce').tolist()\n",
    "for column in hysterectomy_dataset.columns:\n",
    "    if  re.search(r'task_id', column):\n",
    "        lin_hysterectomy = lin_hysterectomy.drop(columns=column)\n",
    "for column in lobectomy_dataset.columns:\n",
    "    if  re.search(r'task_id', column):\n",
    "        lin_lobectomy = lin_lobectomy.drop(columns=column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Hysterectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output variables\n",
    "X = task_relset.drop(columns=['Length of Stay (days)', 'session_name'])\n",
    "y = task_relset['Length of Stay (days)']\n",
    "\n",
    "\n",
    "# Initialize linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive values\n",
    "cv_scores = -cv_scores\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Calculate mean and standard deviation of cross-validation scores\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "std_cv_score = np.std(cv_scores)\n",
    "print(\"Mean CV Score:\", mean_cv_score)\n",
    "print(\"Standard Deviation of CV Scores:\", std_cv_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.1 Hysterectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Define input and output variables\n",
    "X = task_relset.drop(columns=['Length of Stay (days)', 'session_name'])\n",
    "y = task_relset['Length of Stay (days)']\n",
    "\n",
    "# Replace missing values with zeros\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SVM regressor\n",
    "svm_regressor = SVR(kernel='linear')\n",
    "\n",
    "# Train the SVM regressor\n",
    "svm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = svm_regressor.predict(X_test)\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Hysterectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define input and output variables\n",
    "X = task_relset.drop(columns=['Length of Stay (days)', 'session_name'])\n",
    "y = task_relset['Length of Stay (days)']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Decision Tree regressor\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "\n",
    "# Train the Decision Tree regressor\n",
    "tree_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = tree_regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Hysterectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = lin_hysterectomy.drop(columns=['Length of Stay (days)', 'task_name', 'session_name'])\n",
    "y = lin_hysterectomy['Length of Stay (days)']\n",
    "\n",
    "# Replace missing values with zeros\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest regressor\n",
    "forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest regressor\n",
    "forest_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = forest_regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Hysterectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define input and output variables\n",
    "X = task_relset.drop(columns=['Length of Stay (days)', 'session_name'])\n",
    "y = task_relset['Length of Stay (days)']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize MLPRegressor with a single hidden layer\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(13,100,1), activation='relu', solver='adam', max_iter=100, random_state=42)\n",
    "\n",
    "# Train the MLPRegressor\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = mlp_regressor.predict(X_test)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datasets and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing and sheets returned.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import join\n",
    "import sklearn\n",
    "import re\n",
    "\n",
    "# specify the path to file\n",
    "joiner = join.joiner(\"D:\\Projects\\Data Analysis\\Robot Assisted surgery EDA\\RCS_X_Xi_metrics_report_with_LoS.xlsx\")\n",
    "sheets = joiner.get_sheets()\n",
    "metrics = sheets['X_Xi_metrics_report']\n",
    "los_data = sheets['X_Xi_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surgereis in metrics :['hysterectomy' 'lobectomy']\n",
      "Surgeries in los sheet:['hysterectomy' 'lobectomy']\n"
     ]
    }
   ],
   "source": [
    "# - data feature columns of interest\n",
    "feature_col = ['session_name','proc_type','task_name','task_id','time_start','time_stop','evtcnt_camera_control','evtcnt_head_out','energy_totalduration','eom_usm1','eom_usm2','eom_usm3','eom_usm4']\n",
    "metrics = metrics[feature_col]\n",
    "\n",
    "# extract dataframes for hysterectomy and lobectomy using boolean indexing\n",
    "suregeries = ['hysterectomy', 'lobectomy']\n",
    "\n",
    "# Using boolean indexing to filter the DataFrame\n",
    "metrics = metrics[metrics['proc_type'].isin(suregeries)]\n",
    "los_data = los_data[los_data['proc_type'].isin(suregeries)]\n",
    "print(f\"Surgereis in metrics :{metrics['proc_type'].unique()}\")   # - confirm that only the metioned surgeries are considered\n",
    "print(f\"Surgeries in los sheet:{los_data['proc_type'].unique()}\") # - confirm that only the metioned surgeries are considered\n",
    "\n",
    "# - get total task duration column from the start and end time.\n",
    "metrics['time_start'] = pd.to_timedelta(metrics['time_start'])\n",
    "metrics['time_stop'] = pd.to_timedelta(metrics['time_stop'])\n",
    "metrics['task_duration'] = (metrics['time_stop'] - metrics['time_start']).dt.total_seconds()\n",
    "\n",
    "# Rearrange column order\n",
    "columns = list(metrics.columns)\n",
    "columns.remove('task_duration')\n",
    "columns.insert(5, 'task_duration')\n",
    "columns.remove('time_start')\n",
    "columns.remove('time_stop')\n",
    "metrics = metrics[columns]\n",
    "\n",
    "# split the dataframes based for different surgereis\n",
    "hysterectomy_metrics = metrics[metrics['proc_type'].isin(['hysterectomy'])]\n",
    "hysterectomy_los = los_data[los_data['proc_type'].isin(['hysterectomy'])]\n",
    "lobectomy_metrics = metrics[metrics['proc_type'].isin(['lobectomy'])]\n",
    "lobectomy_los = los_data[los_data['proc_type'].isin(['lobectomy'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Averaging task and normalising using task duration for LOS prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupy by session name and task id\n",
    "hysterectomy_metrics = hysterectomy_metrics.groupby(['session_name','task_id']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average by task duration\n",
    "col_to_average = ['evtcnt_camera_control','evtcnt_head_out','energy_totalduration','eom_usm1','eom_usm2','eom_usm3','eom_usm4']\n",
    "for col in col_to_average:\n",
    "    hysterectomy_metrics[col] = hysterectomy_metrics[col]/hysterectomy_metrics['task_duration']\n",
    "# drop the task_name column\n",
    "hysterectomy_metrics = hysterectomy_metrics.drop(columns = ['proc_type','task_name','task_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum up to get the session features\n",
    "hysterectomy_metrics = hysterectomy_metrics.groupby(['session_name']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with los data\n",
    "hysterectomy_los = hysterectomy_los[['session_name','Length of Stay (days)']]\n",
    "hysterectomy_dataset = pd.merge(hysterectomy_metrics,hysterectomy_los, on ='session_name',how ='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 preprocess for ML model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the columns to integers and strings\n",
    "strings = [r'session_name']\n",
    "floats = [r'evtcnt_camera_control',r'evtcnt_head_out',r'energy_totalduration',r'eom_usm1',r'eom_usm2',r'eom_usm3',r'eom_usm4']\n",
    "\n",
    "#convert dtypes\n",
    "for column in hysterectomy_dataset.columns:\n",
    "    for patterns in strings:\n",
    "        if re.search(patterns, column):\n",
    "            hysterectomy_dataset[column] = hysterectomy_dataset[column].astype('string')\n",
    "    for patternf in floats:\n",
    "        if re.search(patternf, column):\n",
    "            hysterectomy_dataset[column] = pd.to_numeric(hysterectomy_dataset[column], errors='coerce').tolist()\n",
    "\n",
    "hysterectomy_dataset.to_excel('exp2_session_los.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score,train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Define input and output variables\n",
    "X = hysterectomy_dataset.drop(columns=['Length of Stay (days)', 'session_name'])\n",
    "X = X.values\n",
    "y = hysterectomy_dataset['Length of Stay (days)']\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for current fold: 10.974788254040636\n",
      "Mean Squared Error for current fold: 1.3299471253283872\n",
      "Mean Squared Error for current fold: 11.377181076389226\n",
      "Mean Squared Error for current fold: 298.9213942808207\n",
      "Mean Squared Error for current fold: 13.114063042460067\n",
      "Average Mean Squared Error: 67.14347475580782\n"
     ]
    }
   ],
   "source": [
    "# Initialize the KFold object with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "    print(\"Mean Squared Error for current fold:\", mse)\n",
    "\n",
    "# Calculate the average mean squared error\n",
    "average_mse = np.mean(mse_scores)\n",
    "print(\"Average Mean Squared Error:\", average_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a single fold has a very high mse, which in turn is increasing the mean MSE. This is same for every case even if increase the no. of folds. A sinlge fold has extremely large value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for current fold: 5.833333333333333\n",
      "Mean Squared Error for current fold: 8.363636363636363\n",
      "Mean Squared Error for current fold: 10.909090909090908\n",
      "Average Mean Squared Error: 8.368686868686867\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "    dt_regressor.fit(X_train, y_train)\n",
    "    y_pred = dt_regressor.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "    print(\"Mean Squared Error for current fold:\", mse)\n",
    "    \n",
    "average_mse = np.mean(mse_scores)\n",
    "print(\"Average Mean Squared Error:\", average_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for each fold: [40.33559629758877, 11.079061452130519, 64.83625513037157]\n",
      "Average Mean Squared Error: 38.75030429336362\n"
     ]
    }
   ],
   "source": [
    "svm_df = hysterectomy_dataset\n",
    "svm_df.fillna(0,inplace=True)\n",
    "svm_df.reset_index(drop=True, inplace=True)\n",
    "X_svm = hysterectomy_dataset.drop(columns=['Length of Stay (days)', 'session_name'])\n",
    "X_svm = X_svm.values\n",
    "y_svm = hysterectomy_dataset['Length of Stay (days)']\n",
    "y_svm = y_svm.values\n",
    "\n",
    "svm_regressor = SVR(kernel='linear')\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_svm):\n",
    "    X_train, X_test = X_svm[train_index], X_svm[test_index]\n",
    "    y_train, y_test = y_svm[train_index], y_svm[test_index]\n",
    "    \n",
    "    svm_regressor.fit(X_train, y_train)\n",
    "    y_pred = svm_regressor.predict(X_test)\n",
    "    \n",
    "   \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "\n",
    "average_mse = np.mean(mse_scores)\n",
    "\n",
    "print(\"Mean Squared Error for each fold:\", mse_scores)\n",
    "print(\"Average Mean Squared Error:\", average_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for each fold: [ 7.01563651  0.89605859  2.84847817 17.44275802 11.45231343]\n",
      "Average Mean Squared Error: 7.931048943915345\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize the KFold object with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "mse_scores = cross_val_score(rf_regressor, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the mean squared error scores\n",
    "mse_scores = -mse_scores  # convert negative MSE scores to positive\n",
    "mean_mse = np.mean(mse_scores)\n",
    "\n",
    "print(\"Mean Squared Error for each fold:\", mse_scores)\n",
    "print(\"Average Mean Squared Error:\", mean_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for each fold: [1.64067363 8.93619445 7.00562146]\n",
      "Average Mean Squared Error: 5.860829842980972\n"
     ]
    }
   ],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(7,35,1), activation='relu', solver='adam', max_iter=5000, random_state=42)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "mse_scores = cross_val_score(mlp_regressor, X_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "mse_scores = -mse_scores  \n",
    "mean_mse = np.mean(mse_scores)\n",
    "\n",
    "print(\"Mean Squared Error for each fold:\", mse_scores)\n",
    "print(\"Average Mean Squared Error:\", mean_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reducing the no. of tasks considered for LOS prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing and sheets returned.\n",
      "Surgereis in metrics :['hysterectomy' 'lobectomy']\n",
      "Surgeries in los sheet:['hysterectomy' 'lobectomy']\n",
      "The no. of unique surgeical cases in metrics set is :31\n",
      "The no. of unique tasks present in the entire set is :25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import join\n",
    "import sklearn\n",
    "\n",
    "# specify the path to file\n",
    "joiner = join.joiner(\"D:\\Projects\\Data Analysis\\Robot Assisted surgery EDA\\RCS_X_Xi_metrics_report_with_LoS.xlsx\")\n",
    "sheets = joiner.get_sheets()\n",
    "metrics = sheets['X_Xi_metrics_report']\n",
    "los_data = sheets['X_Xi_counts']\n",
    "# - rename the session identifier col so tha tboth data set have the same col name.\n",
    "metrics.rename(columns={'session_name': 'session_name'}, inplace=True)\n",
    "\n",
    "# - data feature columns of interest\n",
    "feature_col = ['session_name','proc_type','task_name','task_id','time_start','time_stop','evtcnt_camera_control','evtcnt_head_out','energy_totalduration','eom_usm1','eom_usm2','eom_usm3','eom_usm4']\n",
    "metrics = metrics[feature_col]\n",
    "\n",
    "# extract dataframes for hysterectomy and lobectomy using boolean indexing\n",
    "suregeries = ['hysterectomy', 'lobectomy']\n",
    "\n",
    "# Using boolean indexing to filter the DataFrame\n",
    "metrics = metrics[metrics['proc_type'].isin(suregeries)]\n",
    "los_data = los_data[los_data['proc_type'].isin(suregeries)]\n",
    "print(f\"Surgereis in metrics :{metrics['proc_type'].unique()}\")   # - confirm that only the metioned surgeries are considered\n",
    "print(f\"Surgeries in los sheet:{los_data['proc_type'].unique()}\") # - confirm that only the metioned surgeries are considered\n",
    "\n",
    "# - get total task duration column from the start and end time.\n",
    "metrics['time_start'] = pd.to_timedelta(metrics['time_start'])\n",
    "metrics['time_stop'] = pd.to_timedelta(metrics['time_stop'])\n",
    "metrics['task_duration'] = (metrics['time_stop'] - metrics['time_start']).dt.total_seconds()\n",
    "\n",
    "# Rearrange column order\n",
    "columns = list(metrics.columns)\n",
    "columns.remove('task_duration')\n",
    "columns.insert(5, 'task_duration')\n",
    "columns.remove('time_start')\n",
    "columns.remove('time_stop')\n",
    "metrics = metrics[columns]\n",
    "\n",
    "# split the dataframes based for different surgereis\n",
    "hysterectomy_metrics = metrics[metrics['proc_type'].isin(['hysterectomy'])]\n",
    "hysterectomy_los = los_data[los_data['proc_type'].isin(['hysterectomy'])]\n",
    "lobectomy_metrics = metrics[metrics['proc_type'].isin(['lobectomy'])]\n",
    "lobectomy_los = los_data[los_data['proc_type'].isin(['lobectomy'])]\n",
    "\n",
    "# groub by session id,task name and task id\n",
    "hysterectomy_metrics = hysterectomy_metrics.drop(columns=['proc_type'])\n",
    "hysterectomy_metrics = hysterectomy_metrics.groupby(['session_name','task_name','task_id']).sum().reset_index()\n",
    "print(f\"The no. of unique surgeical cases in metrics set is :{len(hysterectomy_metrics['session_name'].unique())}\")\n",
    "print(f\"The no. of unique tasks present in the entire set is :{len(hysterectomy_metrics['task_name'].unique())}\")\n",
    "#print(f\"The frequency tasks is as follows :{hysterectomy_metrics['task_name'].value_counts()}\")\n",
    "\n",
    "# Unique session IDs\n",
    "session_ids = hysterectomy_metrics['session_name'].unique()\n",
    "\n",
    "# List of hysterectomy tasks\n",
    "hysterectomy_tasks = ['division of the broad ligament (right side)', 'bladder flap creation',\n",
    "                      'dissection of ip ligament (right side) (optional)', 'division of the broad ligament (left side)',\n",
    "                      'colpotomy', 'division of uterine vessels (right side)', 'removal of the uterus',\n",
    "                      'vaginal cuff closure']\n",
    "\n",
    "# Create a DataFrame with all combinations of session IDs and tasks\n",
    "all_tasks_df = pd.DataFrame([(sid, task) for sid in session_ids for task in hysterectomy_tasks],\n",
    "                            columns=['session_name', 'task_name'])\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(all_tasks_df)\n",
    "\n",
    "# standardised the procedure set\n",
    "merged_df = pd.merge(all_tasks_df, hysterectomy_metrics, on=['session_name', 'task_name'], how='left')\n",
    "merged_df.fillna(value=np.nan, inplace=True)\n",
    "merged_df = merged_df.groupby(['session_name', 'task_name']).sum().reset_index()\n",
    "\n",
    "\n",
    "# normalise using task duration and drop task duration column\n",
    "columns_to_normalize = ['evtcnt_camera_control', 'evtcnt_head_out', 'energy_totalduration',\n",
    "                        'eom_usm1', 'eom_usm2', 'eom_usm3', 'eom_usm4']\n",
    "for col in columns_to_normalize:\n",
    "    merged_df[col] = merged_df[col] / merged_df['task_duration']\n",
    "merged_df = merged_df.drop(columns='task_duration')\n",
    "\n",
    "# restructure to have sinlge row for each session\n",
    "def restructure(urology):\n",
    "    # restructure data frame, such that each row is a single session id\n",
    "    result = pd.DataFrame()\n",
    "    result_temp = None\n",
    "    i = 0\n",
    "    counter = 0\n",
    "    col_pointer = 0\n",
    "    for session_id in urology['session_name']:\n",
    "        if session_id == urology.iloc[col_pointer,0]:\n",
    "            if i == col_pointer:\n",
    "                result_temp = pd.DataFrame()\n",
    "                df2 = pd.DataFrame(urology.iloc[i,:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            else:\n",
    "                df2 = pd.DataFrame(urology.iloc[i,2:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            i = i +1\n",
    "        else:\n",
    "            result.reset_index(drop=True, inplace=True)\n",
    "            result = pd.concat([result,pd.DataFrame(result_temp)],axis = 0)\n",
    "            col_pointer = i\n",
    "            if i == col_pointer:\n",
    "                result_temp = pd.DataFrame()\n",
    "                df2 = pd.DataFrame(urology.iloc[i,:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            else:\n",
    "                df2 = pd.DataFrame(urology.iloc[i,2:]).T\n",
    "                result_temp.index = df2.index = [0]\n",
    "                result_temp = pd.concat([result_temp,df2],axis=1)\n",
    "            i = i +1\n",
    "    return result\n",
    "\n",
    "restruct_df = restructure(merged_df)\n",
    "\n",
    "# join with los_data\n",
    "hysterectomy_los = hysterectomy_los[['session_name','Length of Stay (days)']]\n",
    "hysterectomy_dataset = pd.merge(restruct_df, hysterectomy_los, on=['session_name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the task_id values are identical for the same task (fill the taskid col with highest occuring value)\n",
    "for col in range(len(hysterectomy_dataset.columns)):\n",
    "    if hysterectomy_dataset.columns[col] == \"task_id\":\n",
    "        value_counts = hysterectomy_dataset.iloc[:,col].value_counts()\n",
    "        most_common_value = value_counts.idxmax()\n",
    "        if most_common_value == 0:\n",
    "            most_common_value = hysterectomy_dataset.iloc[:,col].max()\n",
    "        hysterectomy_dataset.iloc[:,col] = most_common_value\n",
    "\n",
    "# make sure each column in dataframe is uniquely named\n",
    "col_h = hysterectomy_dataset.columns.tolist()\n",
    "for i in range(len(col_h)):\n",
    "    if col_h[i] == \"task_id\":\n",
    "        value_counts = hysterectomy_dataset.iloc[:,i].value_counts()\n",
    "        most_common_value = value_counts.idxmax()\n",
    "        col_h[i] = f\"{most_common_value}\" + \"task_id\"\n",
    "        col_h[i+1] = f\"{most_common_value}\" + \"evtcnt_camera_control\"\n",
    "        col_h[i+2] = f\"{most_common_value}\" + \"evtcnt_head_out\"\n",
    "        col_h[i+3] = f\"{most_common_value}\" + \"energy_totalduration\"\n",
    "        col_h[i+4] = f\"{most_common_value}\" + \"eom_usm1\"\n",
    "        col_h[i+5] = f\"{most_common_value}\" + \"eom_usm2\"\n",
    "        col_h[i+6] = f\"{most_common_value}\" + \"eom_usm3\"\n",
    "        col_h[i+7] = f\"{most_common_value}\" + \"eom_usm4\"\n",
    "hysterectomy_dataset.columns = col_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# define datasets\n",
    "lin_hysterectomy = hysterectomy_dataset\n",
    "\n",
    "# convert the columns to integers and strings\n",
    "strings = [r'session_name',r'task_name',r'task_id']\n",
    "floats = [r'evtcnt_camera_control',r'evtcnt_head_out',r'energy_totalduration',r'eom_usm1',r'eom_usm2',r'eom_usm3',r'eom_usm4']\n",
    "\n",
    "#convert dtypes\n",
    "for column in hysterectomy_dataset.columns:\n",
    "    for patterns in strings:\n",
    "        if re.search(patterns, column):\n",
    "            hysterectomy_dataset[column] = hysterectomy_dataset[column].astype('string')\n",
    "    for patternf in floats:\n",
    "        if re.search(patternf, column):\n",
    "            hysterectomy_dataset[column] = pd.to_numeric(hysterectomy_dataset[column], errors='coerce').tolist()\n",
    "for column in hysterectomy_dataset.columns:\n",
    "    if  re.search(r'task_id', column):\n",
    "        lin_hysterectomy = lin_hysterectomy.drop(columns=column)\n",
    "lin_hysterectomy = lin_hysterectomy.drop(columns=['task_name'])\n",
    "lin_hysterectomy.to_excel('exp2_limited_feat.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['417.0evtcnt_camera_control', '417.0evtcnt_head_out',\n",
      "       '417.0energy_totalduration', '417.0eom_usm1', '417.0eom_usm2',\n",
      "       '417.0eom_usm3', '417.0eom_usm4', '422.0evtcnt_camera_control',\n",
      "       '422.0evtcnt_head_out', '422.0energy_totalduration', '422.0eom_usm1',\n",
      "       '422.0eom_usm2', '422.0eom_usm3', '422.0eom_usm4',\n",
      "       '406.0evtcnt_camera_control', '406.0evtcnt_head_out',\n",
      "       '406.0energy_totalduration', '406.0eom_usm1', '406.0eom_usm2',\n",
      "       '406.0eom_usm3', '406.0eom_usm4', '416.0evtcnt_camera_control',\n",
      "       '416.0evtcnt_head_out', '416.0energy_totalduration', '416.0eom_usm1',\n",
      "       '416.0eom_usm2', '416.0eom_usm3', '416.0eom_usm4',\n",
      "       '410.0evtcnt_camera_control', '410.0evtcnt_head_out',\n",
      "       '410.0energy_totalduration', '410.0eom_usm1', '410.0eom_usm2',\n",
      "       '410.0eom_usm3', '410.0eom_usm4', '561.0evtcnt_camera_control',\n",
      "       '561.0evtcnt_head_out', '561.0energy_totalduration', '561.0eom_usm1',\n",
      "       '561.0eom_usm2', '561.0eom_usm3', '561.0eom_usm4',\n",
      "       '423.0evtcnt_camera_control', '423.0evtcnt_head_out',\n",
      "       '423.0energy_totalduration', '423.0eom_usm1', '423.0eom_usm2',\n",
      "       '423.0eom_usm3', '423.0eom_usm4', '424.0evtcnt_camera_control',\n",
      "       '424.0evtcnt_head_out', '424.0energy_totalduration', '424.0eom_usm1',\n",
      "       '424.0eom_usm2', '424.0eom_usm3', '424.0eom_usm4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = lin_hysterectomy.drop(columns=['Length of Stay (days)', 'session_name'])\n",
    "X.fillna(0,inplace=True)\n",
    "y = lin_hysterectomy['Length of Stay (days)']\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [48.15771708 10.07900698  7.9465161  10.10057473  7.03121644]\n",
      "Mean CV Score: 16.663006266852527\n",
      "Standard Deviation of CV Scores: 15.792909420400349\n"
     ]
    }
   ],
   "source": [
    "model_lr = LinearRegression()\n",
    "cv_scores_lr = cross_val_score(model_lr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_lr = -cv_scores_lr\n",
    "print(\"Cross-validation scores:\", cv_scores_lr)\n",
    "\n",
    "# Calculate mean and standard deviation of cross-validation scores\n",
    "mean_cv_score_lr = np.mean(cv_scores_lr)\n",
    "std_cv_score_lr = np.std(cv_scores_lr)\n",
    "print(\"Mean CV Score:\", mean_cv_score_lr)\n",
    "print(\"Standard Deviation of CV Scores:\", std_cv_score_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [ 4.96708256  0.95534679 14.03764384]\n",
      "Mean CV Score: 6.653357730964918\n",
      "Standard Deviation of CV Scores: 5.4723101128708835\n"
     ]
    }
   ],
   "source": [
    "model_svm = SVR(kernel='linear')\n",
    "cv_scores_svm = cross_val_score(model_svm, X, y, cv=3, scoring='neg_mean_squared_error')\n",
    "cv_scores_svm = -cv_scores_svm\n",
    "print(\"Cross-validation scores:\", cv_scores_svm)\n",
    "\n",
    "# Calculate mean and standard deviation of cross-validation scores\n",
    "mean_cv_score_svm = np.mean(cv_scores_svm)\n",
    "std_cv_score_svm = np.std(cv_scores_svm)\n",
    "print(\"Mean CV Score:\", mean_cv_score_svm)\n",
    "print(\"Standard Deviation of CV Scores:\", std_cv_score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [11.17857143  7.71428571  0.71428571  5.875      18.33333333]\n",
      "Mean CV Score: 8.763095238095238\n",
      "Standard Deviation of CV Scores: 5.859153739208636\n"
     ]
    }
   ],
   "source": [
    "model_dt = DecisionTreeRegressor()\n",
    "cv_scores_dt = cross_val_score(model_dt, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_dt = -cv_scores_dt\n",
    "print(\"Cross-validation scores:\", cv_scores_dt)\n",
    "\n",
    "# Calculate mean and standard deviation of cross-validation scores\n",
    "mean_cv_score_dt = np.mean(cv_scores_dt)\n",
    "std_cv_score_dt = np.std(cv_scores_dt)\n",
    "print(\"Mean CV Score:\", mean_cv_score_dt)\n",
    "print(\"Standard Deviation of CV Scores:\", std_cv_score_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [13.06657254  7.15328571  1.24536639  9.82584843 19.33751123]\n",
      "Mean CV Score: 10.125716860052911\n",
      "Standard Deviation of CV Scores: 6.021489517829252\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "cv_scores_rf = cross_val_score(model_rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_rf = -cv_scores_rf\n",
    "print(\"Cross-validation scores:\", cv_scores_rf)\n",
    "\n",
    "# Calculate mean and standard deviation of cross-validation scores\n",
    "mean_cv_score_rf = np.mean(cv_scores_rf)\n",
    "std_cv_score_rf = np.std(cv_scores_rf)\n",
    "print(\"Mean CV Score:\", mean_cv_score_rf)\n",
    "print(\"Standard Deviation of CV Scores:\", std_cv_score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [41.96199277 12.75227089]\n",
      "Mean CV Score: 27.357131830896023\n",
      "Standard Deviation of CV Scores: 14.604860938363451\n"
     ]
    }
   ],
   "source": [
    "model_nn = MLPRegressor(hidden_layer_sizes=(98,), activation='relu', solver='adam', max_iter=5000, random_state=42)\n",
    "cv_scores_nn = cross_val_score(model_nn, X, y, cv=2, scoring='neg_mean_squared_error')\n",
    "cv_scores_nn = -cv_scores_nn\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores_nn)\n",
    "mean_cv_score_nn = np.mean(cv_scores_nn)\n",
    "std_cv_score_nn = np.std(cv_scores_nn)\n",
    "\n",
    "print(\"Mean CV Score:\", mean_cv_score_nn)\n",
    "print(\"Standard Deviation of CV Scores:\", std_cv_score_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
